# Conversation Message Compression

## Overview

The `Conversation` class supports message compression to keep long chats efficient by summarizing older parts of the conversation and preserving recent context.

Benefits:
- Manage token limits in long-running chats
- Reduce costs while retaining important context
- Keep the conversation focused on the latest exchanges

---

## Basic Usage

```csharp
using LlmTornado;
using LlmTornado.Chat;
using LlmTornado.Chat.Models;

var api = new TornadoApi("your-api-key");
var conversation = api.Chat.CreateConversation(new ChatRequest
{
    Model = ChatModel.OpenAi.Gpt4.Turbo
});

// Build up a long conversation
conversation.AddSystemMessage("You are a helpful assistant.");
conversation.AddUserMessage("Tell me about quantum computing.");
await conversation.GetResponseRich();
conversation.AddUserMessage("What are qubits?");
await conversation.GetResponseRich();
// ... many more exchanges ...

// Compress older messages, keeping the last 5 messages intact (default)
int compressedCount = await conversation.CompressMessages();
Console.WriteLine($"Compressed {compressedCount} messages");
```

---

## Advanced Options

### Custom Chunk Size
Control how messages are grouped for summarization:

```csharp
// Use larger chunks (default is ~10,000 characters)
await conversation.CompressMessages(chunkSize: 15_000);

// Use smaller chunks for more granular summaries
await conversation.CompressMessages(chunkSize: 5_000);
```

### Preserve More Recent Messages
Keep more recent messages without summarization:

```csharp
// Keep the last 10 messages intact (default is 5)
await conversation.CompressMessages(preserveRecentCount: 10);

// Only summarize if there are many old messages
await conversation.CompressMessages(preserveRecentCount: 20);
```

### Control System Message Handling
```csharp
// Don't preserve system messages (include them in summaries)
await conversation.CompressMessages(preserveSystemMessages: false);

// Preserve system messages (default behavior)
await conversation.CompressMessages(preserveSystemMessages: true);
```

### Use Different Model for Summarization
```csharp
// Use a cheaper/faster model for summarization
await conversation.CompressMessages(
    summaryModel: ChatModel.OpenAi.Gpt35.Turbo
);

// Use the same model as the conversation (default)
await conversation.CompressMessages(summaryModel: null);
```

### Custom Summary Prompt
```csharp
// Provide a custom prompt for summarization
await conversation.CompressMessages(
    summaryPrompt: "Create a brief summary of this conversation, focusing on key details and decisions:"
);
```

---

## Automatic Compression with Strategies
Automatically trigger compression using a strategy. Implementations decide when to compress based on message count, character length, periodic turns, or adaptive heuristics.

```csharp
using LlmTornado.Chat;
using LlmTornado.Chat.Models;

var api = new TornadoApi("your-api-key");
var conversation = api.Chat.CreateConversation(new ChatRequest
{
    Model = ChatModel.OpenAi.Gpt4.Turbo
});

// Example: compress when there are too many messages
conversation.CompressionStrategy = new MessageCountCompressionStrategy(
    messageThreshold: 15,
    options: new ConversationCompressionOptions
    {
        PreserveRecentCount = 6,
        ChunkSize = 8000,
        SummaryModel = ChatModel.OpenAi.Gpt35.Turbo
    }
);

conversation.AddSystemMessage("You are a helpful assistant.");

for (int i = 0; i < 20; i++)
{
    conversation.AddUserMessage($"Tell me fact #{i + 1}");
    bool compressed = await conversation.CheckAndCompress(); // triggers according to strategy
    conversation.AddAssistantMessage($"Fact #{i + 1}: [Simulated response]");
}
```

Other built-in strategies you can use similarly:
- `CharacterCountCompressionStrategy` – based on total character length
- `PeriodicCompressionStrategy` – compress every N messages/turns
- `AdaptiveCompressionStrategy` – combines multiple factors (message count + character count)

---

## On-Demand Compression via CheckAndCompress
`CheckAndCompress()` consults the configured `CompressionStrategy` and triggers compression only when needed.

```csharp
// Configure a strategy once
conversation.CompressionStrategy = new CharacterCountCompressionStrategy(
    characterThreshold: 25_000,
    options: new ConversationCompressionOptions
    {
        PreserveRecentCount = 5,
        ChunkSize = 10_000,
        SummaryModel = ChatModel.OpenAi.Gpt35.Turbo
    }
);

// During the conversation lifecycle
conversation.AddUserMessage("A long message...");
await conversation.CheckAndCompress(); // compresses only if threshold is exceeded
```

If you don’t set a `CompressionStrategy`, `CheckAndCompress()` returns `false` and does nothing.

---

## Custom Summarizer
Use a custom summarizer to control how summaries are constructed. Implement `IConversationSummarizer` and set `conversation.Summarizer`.

```csharp
internal class CustomStructuredSummarizer : IConversationSummarizer
{
    public async Task<List<ChatMessage>> SummarizeMessages(
        List<ChatMessage> messages,
        ConversationCompressionOptions options,
        CancellationToken token = default)
    {
        // Simple example: produce a single structured assistant message
        var topics = messages
            .Where(m => m.Role == ChatMessageRoles.User)
            .Select(m => Conversation.GetMessageContent(m))
            .ToList();

        string content = "[Previous conversation summary]:\n" +
                         "- Topics discussed:\n" +
                         string.Join("\n", topics.Select((t, i) => $"  {i + 1}. {t}"));

        return new List<ChatMessage>
        {
            new ChatMessage(ChatMessageRoles.Assistant, content)
        };
    }
}

// Usage
conversation.Summarizer = new CustomStructuredSummarizer();
int compressed = await conversation.CompressMessages(new ConversationCompressionOptions
{
    ChunkSize = 6000,
    PreserveRecentCount = 4,
    PreserveSystemMessages = true,
    SummaryModel = ChatModel.OpenAi.Gpt35.Turbo,
    SummaryPrompt = "Create a structured summary with key topics and decisions:"
});
```

---

## Complete Example

```csharp
using LlmTornado;
using LlmTornado.Chat;
using LlmTornado.Chat.Models;

var api = new TornadoApi("your-api-key");
var conversation = api.Chat.CreateConversation(new ChatRequest
{
    Model = ChatModel.OpenAi.Gpt4.Turbo,
    MaxTokens = 1000
});

conversation.AddSystemMessage("You are a knowledgeable science tutor.");

for (int i = 0; i < 20; i++)
{
    conversation.AddUserMessage($"Tell me fact #{i + 1} about physics.");
    await conversation.GetResponseRich();
}

Console.WriteLine($"Messages before compression: {conversation.Messages.Count}");

int compressed = await conversation.CompressMessages(
    chunkSize: 8000,              // Group ~8k chars per chunk
    preserveRecentCount: 8,       // Keep last 8 messages intact
    preserveSystemMessages: true, // Keep system message
    summaryModel: ChatModel.OpenAi.Gpt35.Turbo, // Use cheaper model for summaries
    summaryPrompt: "Summarize this educational conversation concisely:"
);

Console.WriteLine($"Messages after compression: {conversation.Messages.Count}");
Console.WriteLine($"Compression resulted in {compressed} net change");

conversation.AddUserMessage("Can you recap what we've discussed?");
var response = await conversation.GetResponseRich();
Console.WriteLine($"AI: {response.Text}");
```

---

## How It Works

1. Message categorization
   - System messages (preserved if `PreserveSystemMessages = true`)
   - Recent messages (last N messages defined by `PreserveRecentCount`)
   - Older messages to compress
2. Chunking by approximate character length (`ChunkSize`)
3. Parallel summarization of chunks using the LLM (cheaper or same model)
4. Reconstruction of conversation: system messages + summaries + recent messages

---

## Best Practices

- Compress periodically for long sessions (e.g., every 8–15 turns)
- Use `ChatModel.OpenAi.Gpt35.Turbo` (or another affordable model) for summaries
- Keep enough recent messages to maintain flow (e.g., 6–10)
- Preserve system messages to maintain consistent behavior

Example periodic compression:
```csharp
const int CompressionInterval = 8;
int turn = 0;

foreach (var query in userQueries)
{
    turn++;
    conversation.AddUserMessage(query);
    var resp = await conversation.GetResponseRich();

    if (turn % CompressionInterval == 0)
    {
        await conversation.CompressMessages(
            chunkSize: 6000,
            preserveRecentCount: 6,
            summaryModel: ChatModel.OpenAi.Gpt35.Turbo
        );
    }
}
```

---

## Performance Considerations

- Summaries are generated in parallel per chunk (`Task.WhenAll`), improving speed
- Multiple summarization requests can happen concurrently
- Summaries typically use a lower temperature and bounded token count for consistency
- Consider character thresholds when messages can be long

---

## Limitations

- Summaries may omit nuance present in the original messages
- Tool calls and function results inside compressed segments are not preserved verbatim
- Non-text content (images, audio) in compressed segments is not retained

---

## API Surface (Key Members)

- `Conversation.CompressMessages(ConversationCompressionOptions options)`
- `Conversation.CompressMessages(int chunkSize = 10000, int preserveRecentCount = 5, bool preserveSystemMessages = true, ChatModel? summaryModel = null, string? summaryPrompt = null, int maxSummaryTokens = 1000, CancellationToken token = default)`
- `Conversation.CheckAndCompress(CancellationToken token = default)`
- `Conversation.CompressionStrategy` (set to an `IConversationCompressionStrategy` implementation)
- `Conversation.Summarizer` (set to an `IConversationSummarizer` implementation)
- `Conversation.GetMessageLength(ChatMessage message)` (helper)
- `Conversation.GetMessageContent(ChatMessage message)` (helper)

Strategy interface:
- `interface IConversationCompressionStrategy`
  - `bool ShouldCompress(Conversation conversation)`
  - `ConversationCompressionOptions GetCompressionOptions(Conversation conversation)`

Summarizer interface:
- `interface IConversationSummarizer`
  - `Task<List<ChatMessage>> SummarizeMessages(List<ChatMessage> messages, ConversationCompressionOptions options, CancellationToken token = default)`

---

## Related Topics

- [Chat Basics](../1.%20basics.md)
- [Message Management](../2.%20messages.md)
- [Streaming](../6.%20streaming.md)
