# Conversation Message Compression

## Overview

The `CompressMessages` method allows you to compress long conversation histories by summarizing older messages while preserving recent context. This is useful for:

- Managing token limits in long conversations
- Reducing API costs while maintaining context
- Keeping conversations focused on recent interactions

## Basic Usage

```csharp
using LlmTornado;
using LlmTornado.Chat;

var api = new TornadoApi("your-api-key");
var conversation = api.Chat.CreateConversation(new ChatRequest
{
    Model = ChatModel.OpenAi.Gpt4.Turbo
});

// Build up a long conversation
conversation.AddSystemMessage("You are a helpful assistant.");
conversation.AddUserMessage("Tell me about quantum computing.");
await conversation.GetResponseRich();
conversation.AddUserMessage("What are qubits?");
await conversation.GetResponseRich();
// ... many more exchanges ...

// Compress older messages, keeping the last 5 messages intact
int compressedCount = await conversation.CompressMessages();
Console.WriteLine($"Compressed {compressedCount} messages");
```

## Advanced Options

### Custom Chunk Size

Control how messages are grouped for summarization:

```csharp
// Use larger chunks (default is 10,000 characters)
await conversation.CompressMessages(chunkSize: 15000);

// Use smaller chunks for more granular summaries
await conversation.CompressMessages(chunkSize: 5000);
```

### Preserve More Recent Messages

Keep more recent messages without summarization:

```csharp
// Keep the last 10 messages intact (default is 5)
await conversation.CompressMessages(preserveRecentCount: 10);

// Only summarize if there are many old messages
await conversation.CompressMessages(preserveRecentCount: 20);
```

### Control System Message Handling

```csharp
// Don't preserve system messages (include them in summaries)
await conversation.CompressMessages(preserveSystemMessages: false);

// Preserve system messages (default behavior)
await conversation.CompressMessages(preserveSystemMessages: true);
```

### Use Different Model for Summarization

```csharp
// Use a cheaper/faster model for summarization
await conversation.CompressMessages(
    summaryModel: ChatModel.OpenAi.Gpt35.Turbo
);

// Use the same model as the conversation (default)
await conversation.CompressMessages(summaryModel: null);
```

### Custom Summary Prompt

```csharp
// Provide a custom prompt for summarization
await conversation.CompressMessages(
    summaryPrompt: "Create a brief summary of this conversation, focusing on technical details and key decisions:"
);
```

## Complete Example

```csharp
using LlmTornado;
using LlmTornado.Chat;

var api = new TornadoApi("your-api-key");
var conversation = api.Chat.CreateConversation(new ChatRequest
{
    Model = ChatModel.OpenAi.Gpt4.Turbo,
    MaxTokens = 1000
});

// Start conversation
conversation.AddSystemMessage("You are a knowledgeable science tutor.");

// Simulate a long conversation
for (int i = 0; i < 20; i++)
{
    conversation.AddUserMessage($"Tell me fact #{i + 1} about physics.");
    await conversation.GetResponseRich();
}

Console.WriteLine($"Messages before compression: {conversation.Messages.Count}");

// Compress with custom settings
int compressed = await conversation.CompressMessages(
    chunkSize: 8000,              // Group ~8k chars per chunk
    preserveRecentCount: 8,       // Keep last 8 messages intact
    preserveSystemMessages: true, // Keep system message
    summaryModel: ChatModel.OpenAi.Gpt35.Turbo, // Use cheaper model for summaries
    summaryPrompt: "Summarize this educational conversation concisely:"
);

Console.WriteLine($"Messages after compression: {conversation.Messages.Count}");
Console.WriteLine($"Compression resulted in {compressed} net change");

// Continue conversation with compressed history
conversation.AddUserMessage("Can you recap what we've discussed?");
var response = await conversation.GetResponseRich();
Console.WriteLine($"AI: {response.Text}");
```

## How It Works

1. **Message Categorization**: Messages are divided into:
   - System messages (preserved if `preserveSystemMessages = true`)
   - Recent messages (last N messages, controlled by `preserveRecentCount`)
   - Older messages to compress

2. **Chunking**: Older messages are grouped into chunks based on character count (`chunkSize`)

3. **Parallel Summarization**: Each chunk is summarized in parallel using the AI model

4. **Reconstruction**: The conversation is rebuilt with:
   - System messages (if preserved)
   - Summary messages (marked as Assistant messages with "[Previous conversation summary]" prefix)
   - Recent messages (preserved as-is)

## Best Practices

### When to Compress

```csharp
// Check message count before compressing
if (conversation.Messages.Count > 30)
{
    await conversation.CompressMessages(preserveRecentCount: 10);
}

// Or check total character length
int totalLength = conversation.Messages.Sum(m => 
    (m.Content?.Length ?? 0) + 
    (m.Parts?.Sum(p => p.Text?.Length ?? 0) ?? 0)
);

if (totalLength > 50000)
{
    await conversation.CompressMessages(chunkSize: 10000);
}
```

### Regular Compression for Long Conversations

```csharp
int turnCount = 0;
const int CompressionInterval = 15;

while (true)
{
    // User input
    string input = Console.ReadLine();
    if (string.IsNullOrEmpty(input)) break;
    
    conversation.AddUserMessage(input);
    var response = await conversation.GetResponseRich();
    Console.WriteLine($"AI: {response.Text}");
    
    turnCount++;
    
    // Compress periodically
    if (turnCount % CompressionInterval == 0)
    {
        await conversation.CompressMessages(
            preserveRecentCount: 6,
            summaryModel: ChatModel.OpenAi.Gpt35.Turbo
        );
        Console.WriteLine("[Conversation compressed]");
    }
}
```

### Error Handling

```csharp
try
{
    int compressed = await conversation.CompressMessages();
    Console.WriteLine($"Successfully compressed {compressed} messages");
}
catch (Exception ex)
{
    Console.WriteLine($"Compression failed: {ex.Message}");
    // Conversation remains unchanged if compression fails
}
```

## Performance Considerations

- **Parallel Processing**: Chunks are summarized in parallel, making compression fast even for long conversations
- **Network Efficiency**: Multiple summarization requests happen simultaneously
- **Token Optimization**: Summaries use lower temperature (0.3) and limited tokens (500) for consistency
- **Cost Savings**: Use a cheaper model for summarization while keeping a powerful model for the main conversation

## Limitations

- Compression creates summary messages that may lose some nuance from the original conversation
- System messages marked as preserved are always kept, regardless of their position
- Tool calls and function results in compressed messages are not explicitly preserved in summaries
- Images and other non-text content in compressed messages are lost (only text is summarized)

## Related Topics

- [Chat Basics](../../../LlmTornado.Docs/website/docs/1.%20LlmTornado/Chat/1.%20basics.md)
- [Message Management](../../../LlmTornado.Docs/website/docs/1.%20LlmTornado/Chat/2.%20messages.md)
- [Streaming](../../../LlmTornado.Docs/website/docs/1.%20LlmTornado/Chat/6.%20streaming.md)
